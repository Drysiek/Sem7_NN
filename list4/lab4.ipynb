{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "\n",
    "X = heart_disease.data.features\n",
    "y = heart_disease.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting rows with NAN\n",
    "X['y']=y\n",
    "X=X.dropna(how='any')\n",
    "y=X['y']\n",
    "X=X.drop('y', axis=1)\n",
    "\n",
    "# replace disease stadium with one information- sick\n",
    "y = y.replace([2, 3, 4], 1)\n",
    "\n",
    "# Transforming categorical values into binary, one-hot encoded values\n",
    "X = pandas.get_dummies(X, columns=['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal'], dtype=float)\n",
    "\n",
    "# normalize data\n",
    "normalizer = Normalizer().fit(X)\n",
    "X_nm = normalizer.transform(X)\n",
    "X_nm = pandas.DataFrame(X_nm, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_nm, y, test_size=0.25, random_state=65)\n",
    "\n",
    "# preparing torch tensor datasets\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float)\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.float)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float)\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.float)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_sizes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        prev_layer_output_size = input_size \n",
    "\n",
    "        # creating hidden layers\n",
    "        for layer_size in hidden_layer_sizes:\n",
    "            hidden_layer = nn.Linear(prev_layer_output_size, layer_size)\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "            prev_layer_output_size = layer_size\n",
    "        print(self.hidden_layers)\n",
    "            \n",
    "        # output layer\n",
    "        self.output_layer = nn.Linear(prev_layer_output_size, 1)\n",
    "        \n",
    "        # activation function for hidden layers\n",
    "        self.activation_function = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # coming through hiddden layers\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_function(hidden_layer(x))\n",
    "        \n",
    "        # output calculated with sigmoid\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, epochs=2000):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, data_loader):\n",
    "    with torch.no_grad():\n",
    "        Y_true, Y_pred = [], []\n",
    "        for features, targets in data_loader:\n",
    "            outputs = model(features)\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            Y_true.extend(targets.tolist())\n",
    "            Y_pred.extend(predicted.tolist())\n",
    "    print('Accuracy:', accuracy_score(Y_true, Y_pred), 'Precision:', precision_score(Y_true, Y_pred), 'F1-Score:', f1_score(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Linear(in_features=25, out_features=16, bias=True)\n",
      "  (1): Linear(in_features=16, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train.shape[1]\n",
    "hidden_layers = [16, 7]\n",
    "model = NeuralNetwork(input_size, hidden_layers)\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "batch_sizes = [3, 5, 10, 20] \n",
    "optimizers = [torch.optim.SGD, torch.optim.Adam, torch.optim.RMSprop]\n",
    "O_names = [\"SGD\", \"Adam\", \"RMSprop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(lr, opt_name, optimizer, batch_size, epochs):\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # model = NeuralNetwork(input_size, hidden_layers)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    print(opt_name, \"Learning rate:\", lr, \"Batch:\", batch_size)\n",
    "    train_model(model, criterion, optimizer(model.parameters(), lr=lr), train_loader, epochs)\n",
    "    evaluate_metrics(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Learning rate: 0.0001 Batch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\505ry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.52 Precision: 0.0 F1-Score: 0.0\n",
      "SGD Learning rate: 0.0001 Batch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\505ry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.52 Precision: 0.0 F1-Score: 0.0\n",
      "SGD Learning rate: 0.0001 Batch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\505ry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.52 Precision: 0.0 F1-Score: 0.0\n",
      "SGD Learning rate: 0.0001 Batch: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\505ry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.52 Precision: 0.0 F1-Score: 0.0\n",
      "Adam Learning rate: 0.0001 Batch: 3\n",
      "Accuracy: 0.84 Precision: 0.9 F1-Score: 0.8181818181818182\n",
      "Adam Learning rate: 0.0001 Batch: 5\n",
      "Accuracy: 0.8266666666666667 Precision: 0.896551724137931 F1-Score: 0.7999999999999999\n",
      "Adam Learning rate: 0.0001 Batch: 10\n",
      "Accuracy: 0.8266666666666667 Precision: 0.896551724137931 F1-Score: 0.7999999999999999\n",
      "Adam Learning rate: 0.0001 Batch: 20\n",
      "Accuracy: 0.8266666666666667 Precision: 0.896551724137931 F1-Score: 0.7999999999999999\n",
      "RMSprop Learning rate: 0.0001 Batch: 3\n",
      "Accuracy: 0.8533333333333334 Precision: 0.9032258064516129 F1-Score: 0.835820895522388\n",
      "RMSprop Learning rate: 0.0001 Batch: 5\n",
      "Accuracy: 0.8533333333333334 Precision: 0.9032258064516129 F1-Score: 0.835820895522388\n",
      "RMSprop Learning rate: 0.0001 Batch: 10\n",
      "Accuracy: 0.8533333333333334 Precision: 0.9032258064516129 F1-Score: 0.835820895522388\n",
      "RMSprop Learning rate: 0.0001 Batch: 20\n",
      "Accuracy: 0.8533333333333334 Precision: 0.9032258064516129 F1-Score: 0.835820895522388\n",
      "SGD Learning rate: 0.001 Batch: 3\n",
      "Accuracy: 0.84 Precision: 0.9 F1-Score: 0.8181818181818182\n",
      "SGD Learning rate: 0.001 Batch: 5\n",
      "Accuracy: 0.8533333333333334 Precision: 0.9032258064516129 F1-Score: 0.835820895522388\n",
      "SGD Learning rate: 0.001 Batch: 10\n",
      "Accuracy: 0.8 Precision: 0.8888888888888888 F1-Score: 0.761904761904762\n",
      "SGD Learning rate: 0.001 Batch: 20\n",
      "Accuracy: 0.8533333333333334 Precision: 0.9032258064516129 F1-Score: 0.835820895522388\n",
      "Adam Learning rate: 0.001 Batch: 3\n",
      "Accuracy: 0.8266666666666667 Precision: 0.8484848484848485 F1-Score: 0.8115942028985507\n",
      "Adam Learning rate: 0.001 Batch: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\505ry\\Desktop\\sieci neuronowe\\list4\\lab4- fin — kopia.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/505ry/Desktop/sieci%20neuronowe/list4/lab4-%20fin%20%E2%80%94%20kopia.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m opt_name, optimizer \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(O_names, optimizers):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/505ry/Desktop/sieci%20neuronowe/list4/lab4-%20fin%20%E2%80%94%20kopia.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_size \u001b[39min\u001b[39;00m batch_sizes:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/505ry/Desktop/sieci%20neuronowe/list4/lab4-%20fin%20%E2%80%94%20kopia.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         training(lr, opt_name, optimizer, batch_size)\n",
      "\u001b[1;32mc:\\Users\\505ry\\Desktop\\sieci neuronowe\\list4\\lab4- fin — kopia.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/505ry/Desktop/sieci%20neuronowe/list4/lab4-%20fin%20%E2%80%94%20kopia.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCELoss()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/505ry/Desktop/sieci%20neuronowe/list4/lab4-%20fin%20%E2%80%94%20kopia.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(opt_name, \u001b[39m\"\u001b[39m\u001b[39mLearning rate:\u001b[39m\u001b[39m\"\u001b[39m, lr, \u001b[39m\"\u001b[39m\u001b[39mBatch:\u001b[39m\u001b[39m\"\u001b[39m, batch_size)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/505ry/Desktop/sieci%20neuronowe/list4/lab4-%20fin%20%E2%80%94%20kopia.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_model(model, criterion, optimizer(model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49mlr), train_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/505ry/Desktop/sieci%20neuronowe/list4/lab4-%20fin%20%E2%80%94%20kopia.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m evaluate_metrics(model, test_loader)\n",
      "\u001b[1;32mc:\\Users\\505ry\\Desktop\\sieci neuronowe\\list4\\lab4- fin — kopia.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/505ry/Desktop/sieci%20neuronowe/list4/lab4-%20fin%20%E2%80%94%20kopia.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/505ry/Desktop/sieci%20neuronowe/list4/lab4-%20fin%20%E2%80%94%20kopia.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/505ry/Desktop/sieci%20neuronowe/list4/lab4-%20fin%20%E2%80%94%20kopia.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n",
      "File \u001b[1;32mc:\\Users\\505ry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\505ry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\505ry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     adam(\n\u001b[0;32m    164\u001b[0m         params_with_grad,\n\u001b[0;32m    165\u001b[0m         grads,\n\u001b[0;32m    166\u001b[0m         exp_avgs,\n\u001b[0;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    169\u001b[0m         state_steps,\n\u001b[0;32m    170\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    172\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    173\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    175\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    176\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    177\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    178\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    179\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    180\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    181\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    182\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\505ry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m func(params,\n\u001b[0;32m    312\u001b[0m      grads,\n\u001b[0;32m    313\u001b[0m      exp_avgs,\n\u001b[0;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    316\u001b[0m      state_steps,\n\u001b[0;32m    317\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    318\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    319\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    320\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    321\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    322\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    323\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    324\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    325\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    326\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    327\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\505ry\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:385\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    384\u001b[0m exp_avg\u001b[39m.\u001b[39mlerp_(grad, \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m--> 385\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad\u001b[39m.\u001b[39;49mconj(), value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n\u001b[0;32m    388\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    for opt_name, optimizer in zip(O_names, optimizers):\n",
    "        for batch_size in batch_sizes:\n",
    "            training(lr, opt_name, optimizer, batch_size, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
