{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, train_size):\n",
    "        self.data = datasets.FashionMNIST(data_path, download=True, transform=transforms.ToTensor())\n",
    "        self.train_size = train_size\n",
    "        self.split_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        return image, label\n",
    "\n",
    "    def split_dataset(self):\n",
    "        num_train = int(self.train_size * len(self.data))\n",
    "        self.data, _ = torch.utils.data.random_split(self.data, [num_train, len(self.data) - num_train])\n",
    "\n",
    "# Definition of a custom collate function\n",
    "def custom_collate(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return images, labels\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_channels=32, kernel_size=3, pool_size=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, num_channels, kernel_size=kernel_size, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(num_channels * 14 * 14, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluate_model function\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the list to store training losses for each hidden size, batch size, and training set size\n",
    "batch_sizes = [4]\n",
    "batch_size = 10\n",
    "train_size = 0.9\n",
    "train_losses_per_config = []\n",
    "num_epochs = 1\n",
    "\n",
    "num_channels = [10, 20, 100, 200]\n",
    "kernel_sizes = [2, 3, (2, 2), (3, 3)]\n",
    "pool_sizes = [2, (2, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the train_model function\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs):\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0  # Initialize the epoch loss\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Add the batch loss to the epoch loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate the average loss for the epoch\n",
    "        average_epoch_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Append the average epoch loss to the list\n",
    "        train_losses.append(average_epoch_loss)\n",
    "\n",
    "        # Print the average epoch loss\n",
    "        # print(f'Number of Channels: {num_channel}, Kernel Size: {kernel_size}, Batch Size: {batch_size}, Epoch [{epoch + 1}/{num_epochs}], Loss: {average_epoch_loss:.4f}')\n",
    "\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_model(num_channel, kernel_size,  pool_size):\n",
    "    model = SimpleCNN(num_channels= num_channel, kernel_size=kernel_size, pool_size = pool_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    custom_dataset = CustomDataset('path', train_size)\n",
    "    custom_data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "    # Move the model to device (CPU or GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialization of the list to store training losses\n",
    "    # train_losses = []\n",
    "\n",
    "    # Training the model\n",
    "    train_losses_per_config.append(train_model(model, custom_data_loader, criterion, optimizer, device, num_epochs))\n",
    "\n",
    "    # Evaluate the model and calculate metrics\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, custom_data_loader, device)\n",
    "    print(f'Number of Channels: {num_channel}, Kernel Size: {kernel_size}, Pool Size: {pool_size};\\t Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Channels: 10, Kernel Size: 2, Pool Size: 2;\t Accuracy: 0.8790, Precision: 0.8792, Recall: 0.8790, F1 Score: 0.8782\n",
      "Number of Channels: 10, Kernel Size: 2, Pool Size: (2, 2);\t Accuracy: 0.8705, Precision: 0.8785, Recall: 0.8705, F1 Score: 0.8708\n",
      "Number of Channels: 10, Kernel Size: 3, Pool Size: 2;\t Accuracy: 0.8801, Precision: 0.8835, Recall: 0.8801, F1 Score: 0.8774\n",
      "Number of Channels: 10, Kernel Size: 3, Pool Size: (2, 2);\t Accuracy: 0.8893, Precision: 0.8888, Recall: 0.8893, F1 Score: 0.8887\n",
      "Number of Channels: 10, Kernel Size: (2, 2), Pool Size: 2;\t Accuracy: 0.8773, Precision: 0.8774, Recall: 0.8773, F1 Score: 0.8743\n",
      "Number of Channels: 10, Kernel Size: (2, 2), Pool Size: (2, 2);\t Accuracy: 0.8796, Precision: 0.8798, Recall: 0.8796, F1 Score: 0.8788\n",
      "Number of Channels: 10, Kernel Size: (3, 3), Pool Size: 2;\t Accuracy: 0.8935, Precision: 0.8948, Recall: 0.8935, F1 Score: 0.8934\n",
      "Number of Channels: 10, Kernel Size: (3, 3), Pool Size: (2, 2);\t Accuracy: 0.8914, Precision: 0.8910, Recall: 0.8914, F1 Score: 0.8907\n",
      "Number of Channels: 20, Kernel Size: 2, Pool Size: 2;\t Accuracy: 0.8898, Precision: 0.8893, Recall: 0.8898, F1 Score: 0.8874\n",
      "Number of Channels: 20, Kernel Size: 2, Pool Size: (2, 2);\t Accuracy: 0.8936, Precision: 0.8932, Recall: 0.8936, F1 Score: 0.8926\n",
      "Number of Channels: 20, Kernel Size: 3, Pool Size: 2;\t Accuracy: 0.8904, Precision: 0.8930, Recall: 0.8904, F1 Score: 0.8895\n",
      "Number of Channels: 20, Kernel Size: 3, Pool Size: (2, 2);\t Accuracy: 0.8920, Precision: 0.8927, Recall: 0.8920, F1 Score: 0.8906\n",
      "Number of Channels: 20, Kernel Size: (2, 2), Pool Size: 2;\t Accuracy: 0.8825, Precision: 0.8896, Recall: 0.8825, F1 Score: 0.8833\n",
      "Number of Channels: 20, Kernel Size: (2, 2), Pool Size: (2, 2);\t Accuracy: 0.8889, Precision: 0.8894, Recall: 0.8889, F1 Score: 0.8860\n",
      "Number of Channels: 20, Kernel Size: (3, 3), Pool Size: 2;\t Accuracy: 0.8871, Precision: 0.8937, Recall: 0.8871, F1 Score: 0.8866\n",
      "Number of Channels: 20, Kernel Size: (3, 3), Pool Size: (2, 2);\t Accuracy: 0.8916, Precision: 0.8942, Recall: 0.8916, F1 Score: 0.8901\n",
      "Number of Channels: 100, Kernel Size: 2, Pool Size: 2;\t Accuracy: 0.8972, Precision: 0.9041, Recall: 0.8972, F1 Score: 0.8988\n",
      "Number of Channels: 100, Kernel Size: 2, Pool Size: (2, 2);\t Accuracy: 0.9048, Precision: 0.9056, Recall: 0.9048, F1 Score: 0.9026\n",
      "Number of Channels: 100, Kernel Size: 3, Pool Size: 2;\t Accuracy: 0.9161, Precision: 0.9171, Recall: 0.9161, F1 Score: 0.9161\n",
      "Number of Channels: 100, Kernel Size: 3, Pool Size: (2, 2);\t Accuracy: 0.9102, Precision: 0.9129, Recall: 0.9102, F1 Score: 0.9103\n",
      "Number of Channels: 100, Kernel Size: (2, 2), Pool Size: 2;\t Accuracy: 0.9067, Precision: 0.9081, Recall: 0.9067, F1 Score: 0.9070\n",
      "Number of Channels: 100, Kernel Size: (2, 2), Pool Size: (2, 2);\t Accuracy: 0.9076, Precision: 0.9087, Recall: 0.9076, F1 Score: 0.9073\n",
      "Number of Channels: 100, Kernel Size: (3, 3), Pool Size: 2;\t Accuracy: 0.9082, Precision: 0.9172, Recall: 0.9082, F1 Score: 0.9099\n",
      "Number of Channels: 100, Kernel Size: (3, 3), Pool Size: (2, 2);\t Accuracy: 0.9140, Precision: 0.9146, Recall: 0.9140, F1 Score: 0.9129\n",
      "Number of Channels: 200, Kernel Size: 2, Pool Size: 2;\t Accuracy: 0.9103, Precision: 0.9112, Recall: 0.9103, F1 Score: 0.9103\n",
      "Number of Channels: 200, Kernel Size: 2, Pool Size: (2, 2);\t Accuracy: 0.9046, Precision: 0.9054, Recall: 0.9046, F1 Score: 0.9028\n",
      "Number of Channels: 200, Kernel Size: 3, Pool Size: 2;\t Accuracy: 0.9090, Precision: 0.9131, Recall: 0.9090, F1 Score: 0.9054\n",
      "Number of Channels: 200, Kernel Size: 3, Pool Size: (2, 2);\t Accuracy: 0.9134, Precision: 0.9159, Recall: 0.9134, F1 Score: 0.9114\n",
      "Number of Channels: 200, Kernel Size: (2, 2), Pool Size: 2;\t Accuracy: 0.9002, Precision: 0.9053, Recall: 0.9002, F1 Score: 0.9000\n",
      "Number of Channels: 200, Kernel Size: (2, 2), Pool Size: (2, 2);\t Accuracy: 0.9040, Precision: 0.9070, Recall: 0.9040, F1 Score: 0.9042\n",
      "Number of Channels: 200, Kernel Size: (3, 3), Pool Size: 2;\t Accuracy: 0.9072, Precision: 0.9127, Recall: 0.9072, F1 Score: 0.9068\n",
      "Number of Channels: 200, Kernel Size: (3, 3), Pool Size: (2, 2);\t Accuracy: 0.9011, Precision: 0.9053, Recall: 0.9011, F1 Score: 0.9012\n"
     ]
    }
   ],
   "source": [
    "# Iterate over different configurations\n",
    "for num_channel in num_channels:\n",
    "    for kernel_size in kernel_sizes:\n",
    "        for pool_size in pool_sizes:\n",
    "            do_model(num_channel, kernel_size, pool_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przedstawione dane dotyczą wyników działania sieci neuronowej konwolucyjnej w różnych konfiguracjach, ze szczególnym uwzględnieniem różnych kombinacji liczby kanałów, rozmiaru kernela i rozmiaru puli.\n",
    "\n",
    "Wpływ Rozmiaru Jądra:\n",
    "Dla stałej liczby kanałów (10, 20, 100, 200), zwiększanie rozmiaru jądra z 2 do 3 powoduje stałą poprawę większości metryk wydajności.\n",
    "\n",
    "Oddziaływanie Rozmiaru Puli:\n",
    "Ogólnie rzecz biorąc, korzystanie z rozmiaru puli (2, 2) zazwyczaj skutkuje nieco lepszą wydajnością w porównaniu z rozmiarem puli równym 2. Ten trend jest zauważalny dla różnych rozmiarów jądra i liczby kanałów.\n",
    "\n",
    "Wpływ Liczby Kanałów:\n",
    "W miarę zwiększania liczby kanałów występuje mieszanka poprawy i pogorszenia metryk wydajności. Warto zauważyć, że większa liczba kanałów nie zawsze oznacza lepszą wydajność, a konieczne jest znalezienie optymalnego kompromisu.\n",
    "\n",
    "Spójność Między Metrykami:\n",
    "W różnych konfiguracjach występuje stosunkowo spójna wydajność we wszystkich metrykach, co wskazuje na zrównoważony model.\n",
    "\n",
    "\n",
    "Najlepsza ogólna wydajność osiągana jest przy 100 kanałach, rozmiarze jądra 3 i rozmiarze puli 2, ponieważ systematycznie wykazuje wysokie wartości we wszystkich metrykach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
